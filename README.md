# Vision-Transformer-Scratch

Most computer vision models for waste sorting are built with CNNs, but I wanted to see how well a Vision Transformer (ViT) could perform on a small dataset. I implemented a ViT from scratch to classify waste images into different categories using around 2,500 samples. This was less about beating benchmarks and more about testing how transformers adapt when data is limited.

ViTs treat images as sequences of patches, which helps them capture global relationships instead of just local features. But they also tend to rely on large-scale datasets or pretraining to perform at their best. On this dataset, the model achieved about 62% accuracy, which is right in line with what you’d expect when training a ViT from scratch on limited data. While it doesn’t outperform CNNs in this setup, the project gave me valuable insight into both the potential and the constraints of transformer-based vision models, and it opens the door to future experiments with transfer learning and hybrid architectures for waste classification.
