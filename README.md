# Vision-Transformer-Scratch

This project implements a Vision Transformer (ViT) from scratch for classifying waste images into different categories (garbage classification). The dataset used consists of ~2,500 images, making it a relatively small-scale experiment for a transformer-based vision model.

Unlike CNNs, Vision Transformers rely heavily on large datasets for strong performance. With limited training data, the model achieves an accuracy of ~62%, which aligns with the expectation that ViTs require substantial data (or pretraining) to outperform traditional convolutional approaches.
